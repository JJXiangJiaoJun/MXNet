{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手动搭建一个RNN网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gluonbook as gb\n",
    "import math\n",
    "from mxnet import autograd, nd\n",
    "from mxnet.gluon import loss as gloss\n",
    "import time\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(corpus_indices, char_to_idx, idx_to_char,\n",
    "vocab_size) = gb.load_data_jay_lyrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 1. ... 0. 0. 0.]]\n",
       "<NDArray 2x1027 @gpu(0)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.one_hot(nd.array([0, 2],ctx=mx.gpu()), vocab_size,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于输入样本是 [批量大小，时间步长] ， 我们需要将其转化为One-Hot编码，编码后格式为 [时间步长，（批量大小，词典词总数）]，即每个批量都是一个One-hot编码，训练时，有步长次输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ont_hot_encoder(X,size):\n",
    "    return [nd.one_hot(word,size) for word in X.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, (2, 1027))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.arange(10,ctx=mx.gpu()).reshape((2, 5))\n",
    "inputs = ont_hot_encoder(X, vocab_size)\n",
    "len(inputs), inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs,num_hiddens,num_outputs = vocab_size,256,vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = gb.try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():\n",
    "    def _one(shape):\n",
    "        return nd.random.normal(scale=0.01,shape=shape,ctx=ctx)\n",
    "    \n",
    "    #隐藏层参数\n",
    "    W_xh = nd.random.normal(scale=0.01,shape=(num_inputs,num_hiddens),ctx=ctx)\n",
    "    W_hh = nd.random.normal(scale=0.01,shape=(num_hiddens,num_hiddens),ctx=ctx)\n",
    "    b_h = nd.zeros(shape=(num_hiddens,),ctx=ctx)\n",
    "    W_ho = nd.random.normal(scale=0.01,shape=(num_hiddens,num_outputs),ctx=ctx)\n",
    "    b_o = nd.zeros(shape=(num_outputs,),ctx=ctx)\n",
    "    \n",
    "    #申请梯度求导内存\n",
    "    params=[W_xh,W_hh,b_h,W_ho,b_o]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义rnn模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ⾸先定义 init_rnn_state 函数来返回初始化的隐藏状态。它返回由⼀个形状为（批量⼤小，隐藏单元个数）的值为 0 的 NDArray 组成的循环神经⽹络元组。使⽤元组是为了更⽅便处理隐藏状态含有多个 NDArray 的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size,num_hiddens,ctx):\n",
    "    return (nd.zeros(shape=(batch_size,num_hiddens),ctx=ctx),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下面定义rnn的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(input,state,params):\n",
    "    # inputs 和 outputs 皆为 num_steps 个形状为（batch_size， vocab_size）的矩阵。\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, =state\n",
    "    outputs = []\n",
    "    for X in input:\n",
    "        H = nd.relu(nd.dot(X,W_xh)+nd.dot(H,W_hh)+b_h)\n",
    "        Y = nd.dot(H,W_hq)+b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs,(H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=init_rnn_state(X.shape[0],num_hiddens,ctx)\n",
    "input = ont_hot_encoder(X,vocab_size)\n",
    "params = get_params()\n",
    "outputs,new_states = rnn(input,state,params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, (2, 1027), (2, 256))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs),outputs[0].shape,new_states[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义预测函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 函数基于prefix个字符，来预测接下来的num_chars个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(prefix,num_chars,rnn,params,init_rnn_state,\n",
    "                num_hiddens,vocab_size,ctx,idx_to_char,char_to_idx):\n",
    "    #首先预处理输入数据,一个输入换成索引\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    #初始化隐藏城状态\n",
    "    state = init_rnn_state(1,num_hiddens,ctx)  #批量数为1\n",
    "    \n",
    "    for t in range(num_chars+len(prefix)-1):\n",
    "        #将上一次的输出作为这一次的输入\n",
    "        X = ont_hot_encoder(nd.array([output[-1]],ctx=ctx),vocab_size)\n",
    "        #前向运算，获得输出\n",
    "        (Y,state) = rnn(X,state,params)\n",
    "        #将输出追加到最后\n",
    "        if t<len(prefix)-1:\n",
    "            output.append(char_to_idx[prefix[t+1]])\n",
    "        else:\n",
    "            #这里 Y 要取下标0是由于 Y为一个列表，需要取出元素才能运算\n",
    "            output.append(int(Y[0].argmax(axis=1).asscalar()))\n",
    "    \n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开四彿去始草泪剧祭去宠'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rnn('分开', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,\n",
    "ctx, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 裁剪梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * 循环神经网络汇总容易出现梯度衰减或梯度爆炸。所以我们把所有模型参数梯度的元素拼接成一个向量 $\\boldsymbol{g}$,并设置裁剪的阈值为 $\\theta$。裁剪后的梯度为\n",
    " \n",
    " $$\n",
    " \\min \\left({\\frac{\\theta}{||\\boldsymbol{g}||},1}\\right)\\boldsymbol{g}\n",
    " $$\n",
    " \n",
    "的$L_2$范数不超过$\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(params,theta,ctx):\n",
    "    norm = nd.array([0.0],ctx=ctx)\n",
    "    for param in params:\n",
    "        norm+=(param.grad**2).sum()\n",
    "    norm = norm.sqrt().asscalar()\n",
    "    \n",
    "    if norm>theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta/norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 困惑度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 困惑度是对交叉熵损失函数做指数运算后得到的值。特别的：\n",
    "    - 最佳情况下，模型总是把标签类别的概率预测为 1。此时困惑度为 1\n",
    "    - 最坏情况下，模型总是把标签类别的概率预测为 0。此时困惑度为正⽆穷。\n",
    "    - 基线情况下，模型总是预测所有类别的概率都相同。此时困惑度为类别个数。\n",
    "    \n",
    "显然，任何⼀个有效模型的困惑度必须小于类别个数。在本例中，困惑度必须小于词典⼤小 vocab_size。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型训练函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 使用困惑度(perplexity)来评价模型\n",
    "2. 在迭代模型参数前裁剪梯度\n",
    "3. 对时序数据采用不同采样方法将导致隐层状态初始化的不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_and_predict_rnn(rnn,get_params,init_rnn_state,num_hiddens,\n",
    "                         vocab_size,ctx,corpus_indices,idx_to_char,\n",
    "                         char_to_idx,is_random_iter,num_epochs,num_steps,lr,\n",
    "                         clipping_theta,batch_size,pred_period,pred_len,prefixes):\n",
    "    if is_random_iter:\n",
    "        data_iter_fn = gb.data_iter_random\n",
    "    else:\n",
    "        data_iter_fn = gb.data_iter_consecutive\n",
    "    params = get_params()\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if not is_random_iter:             #如果使用相邻采样，则在epoch开始时初始化隐藏层状态\n",
    "            state = init_rnn_state(batch_size,num_hiddens,ctx)\n",
    "        loss_sum,start = 0.0,time.time()\n",
    "        data_iter = data_iter_fn(corpus_indices,batch_size,num_steps,ctx)\n",
    "        for t,(X,y) in enumerate(data_iter):\n",
    "            if is_random_iter:  #如果是随机采样，在每个小批量更新前初始化隐藏层状态\n",
    "                state = init_rnn_state(batch_size,num_hiddens,ctx)\n",
    "            else: # 否则需要使⽤ detach 函数从计算图分离隐藏状态。\n",
    "                for s in state:\n",
    "                    s.detach()\n",
    "            #下面进行训练\n",
    "            with autograd.record():\n",
    "                #首先 one-hot编码\n",
    "                inputs = ont_hot_encoder(X,vocab_size)\n",
    "                # outputs 有 num_steps 个形状为（batch_size， vocab_size）的矩阵。\n",
    "                (outputs,state) = rnn(inputs,state,params)\n",
    "                outputs = nd.concat(*outputs,dim=0)\n",
    "                # Y 的形状是（batch_size， num_steps），转置后再变成⻓度为\n",
    "                # batch * num_steps 的向量，这样跟输出的⾏⼀⼀对应。\n",
    "                y =y.T.reshape((-1,))\n",
    "                l = loss(outputs,y).mean()\n",
    "            l.backward()\n",
    "            grad_clipping(params,clipping_theta,ctx)\n",
    "            gb.sgd(params,lr,1)\n",
    "            loss_sum += l.asscalar()\n",
    "        if (epoch+1) % pred_period==0:\n",
    "            print('epoch %d,perplexity %f,time %.2fsec'%\n",
    "                 (epoch+1,math.exp(loss_sum/(t+1)),time.time()-start))\n",
    "            for prefix in prefixes:\n",
    "                print('-',predict_rnn(\n",
    "                prefix,pred_len,rnn,params,init_rnn_state,num_hiddens,vocab_size,ctx,idx_to_char,char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 1000, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 100, 50, ['分开', '不分开']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机采样进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100,perplexity 17.165624,time 0.50sec\n",
      "- 分开 干什么我有朋友一起 融化的让旧人多 牧草有没有 我马儿有些瘦你久跑像人 你我的让我面疼的可爱女人 \n",
      "- 不分开 漂我的恨界里多苍牧哭有悲害 我给儿这生奏就久的手不放多的爱头有 用括的老斑人 印地正不的牛肉 我说\n",
      "epoch 200,perplexity 3.018563,time 0.49sec\n",
      "- 分开 一直令它心仪的母斑鸠缸牛爷爷玫 我失轻的叹尾 不通里这样的屋内 还底什么我想要 却发现迷了路怎么找\n",
      "- 不分开期 风后将不了口慢  我教的有 它阵莫名念动翰福音现为弥补 心有一双蓝色眼睛斑脸天方决斗 伤养上黑一\n",
      "epoch 300,perplexity 1.780099,time 0.51sec\n",
      "- 分开 一只令它三仪的母斑鸠 印像一阵风 吹完它就走 这样的让我面红的可爱女人 温柔的让我心疼的可爱女人 \n",
      "- 不分开期 我叫你爸 你打我妈 这样的吗干都的晴 随 让午来飞子光 瞎 说们都的风迹 不悔成被忆 不街茶美 \n",
      "epoch 400,perplexity 1.581123,time 0.57sec\n",
      "- 分开 一只令它三仪七百多年 灰袋橱有一点秀逗 猎物让夕它飞被走难进y堂的角度 能知道你前世是狼人还道 你\n",
      "- 不分开扫 然后将过去 慢慢温习 让我爱上你 那场悲剧 是你完美演出的一场戏 宁愿心碎哭泣 再狠狠忘记 你爱\n",
      "epoch 500,perplexity 1.493163,time 0.50sec\n",
      "- 分开 距三已 一手走 我打就这样牵着你的手不放开 爱可不能够永远单纯没有悲哀 你 靠着我的肩膀 你 在我\n",
      "- 不分开期 然后将过去 慢慢温习 让我爱上你 那场悲剧 是你完美演出的一场戏 宁愿心碎哭泣 再狠狠忘记 你爱\n",
      "epoch 600,perplexity 1.421720,time 0.49sec\n",
      "- 分开 一直两双三仪四颗 连成线背著背默默许下心愿 看远方的星是否听的见 手牵手 一步两步三步四步望著天 \n",
      "- 不分开吗 我叫你爸 你打我妈 这样对吗干嘛这样 何必让酒牵鼻子走 瞎 说不能 Ch我抬起头 有我去对医药箱\n",
      "epoch 700,perplexity 1.359437,time 0.50sec\n",
      "- 分开 距今已经三千七百多   纪录那最原始的美丽 纪录第一次遇见的你 Jay Chou  如果用 爱血了\n",
      "- 不分开期 然金葛爬满 慢慢温习 让我爱上你 那场悲剧 是你完美演出的一场戏 宁愿心碎哭泣 再狠狠忘记 你爱\n",
      "epoch 800,perplexity 1.333837,time 0.50sec\n",
      "- 分开 距今已 一步两步三步四颗望著的淡淡 漫说就 你爱我 开你了口 周杰的 女酋长下诅咒 还我骷髅头 这\n",
      "- 不分开扫把的胖女巫 用拉丁文念咒语啦啦呜 她养的黑猫笑起来像哭 啦啦啦呜 对对去纵 不血感碰 没有梦功 不\n",
      "epoch 900,perplexity 1.342455,time 0.51sec\n",
      "- 分开 一步令它三在空百前脸天 看远星 一颗两颗三颗四颗 连成线背著背默默许下心愿 看远方的星是否听的见 \n",
      "- 不分开想 我不能再想 我不 我不 我不能再想你 爱情不觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 \n",
      "epoch 1000,perplexity 1.343815,time 0.48sec\n",
      "- 分开 距今已经截棍 哼哼哈兮 习武之人切记 仁者无敌 是谁在练太极 一身正气 他们儿子我习惯 从小就耳濡\n",
      "- 不分开吗 我叫你爸 你打我妈 这样对吗干嘛这样 何必让酒的鼻子 你说 苦笑常常陪着你 在一起有废 花思定实\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "vocab_size, ctx, corpus_indices, idx_to_char,\n",
    "char_to_idx, True, num_epochs, num_steps, lr,\n",
    "clipping_theta, batch_size, pred_period, pred_len,\n",
    "prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相邻采样进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-2435546ae596>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m                         \u001b[0mchar_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                         \u001b[0mclipping_theta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_period\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                         prefixes)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-aaec6f2a652a>\u001b[0m in \u001b[0;36mtrain_and_predict_rnn\u001b[1;34m(rnn, get_params, init_rnn_state, num_hiddens, vocab_size, ctx, corpus_indices, idx_to_char, char_to_idx, is_random_iter, num_epochs, num_steps, lr, clipping_theta, batch_size, pred_period, pred_len, prefixes)\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mgrad_clipping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclipping_theta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[0mgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mloss_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-19f6d10dd437>\u001b[0m in \u001b[0;36mgrad_clipping\u001b[1;34m(params, theta, ctx)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mnorm\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mnorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1996\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The current array is not a scalar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1978\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1979\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1980\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   1981\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                        vocab_size, ctx, corpus_indices, idx_to_char,\n",
    "                        char_to_idx, False, num_epochs, num_steps, lr,\n",
    "                        clipping_theta, batch_size, pred_period, pred_len,\n",
    "                        prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 没有充分训练的模型好像一直在重复预测，好像是一直记住了前面的词\n",
    "- 是否可以考虑，加入遗忘机制？\n",
    "- 使用Relu作为激活函数之后好像收敛速度加快了很多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
