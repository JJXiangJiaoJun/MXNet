{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare custom datasets for object detection\n",
    "===============================================\n",
    "\n",
    "With GluonCV, we have already provided built-in support for widely used public datasets with zero\n",
    "effort, e.g. `sphx_glr_build_examples_datasets_pascal_voc.py` and `sphx_glr_build_examples_datasets_mscoco.py`.\n",
    "\n",
    "However it is very natural to create a custom dataset of your choice for object detection tasks.\n",
    "\n",
    "This tutorial is intend to provide you some hints to clear the path for you.\n",
    "In practice, feel free to choose whatever method that fits for your use case best.\n",
    "\n",
    "`lst_record_dataset`\n",
    "\n",
    "`pascal_voc_like`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Preferred Object Detection Format for GluonCV and MXNet\n",
    "----------------------------------------------------------\n",
    "Let us walk through some fundamental backgrounds in case you are not familiar with them.\n",
    "\n",
    "Bounding Boxes\n",
    "^^^^^^^^^^^^^^\n",
    "\n",
    "There are multiple ways to organize the label format for object detection task. We will briefly introduce the\n",
    "most widely used: ``bounding box``.\n",
    "\n",
    "GluonCV expect all bounding boxes to be encoded as (xmin, ymin, xmax, ymax), aka (left, top, right, bottom) borders of each object of interest.\n",
    "\n",
    "First of all, let us plot a real image for example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "from gluoncv import utils\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "im_fname = utils.download('https://github.com/dmlc/web-data/blob/master/' +\n",
    "                          'gluoncv/datasets/dog.jpg?raw=true',\n",
    "                          path='dog.jpg')\n",
    "img = mx.image.imread(im_fname)\n",
    "ax = utils.viz.plot_image(img)\n",
    "print(img.shape)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's label the image manually for demo.\n",
    "\n",
    ".. hint::\n",
    "\n",
    "   In practice, a dedicated GUI labeling tool is more convenient.\n",
    "\n",
    "We expect all bounding boxes follow this format: (xmin, ymin, xmax, ymax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_label = [130, 220, 320, 530]\n",
    "bike_label = [115, 120, 580, 420]\n",
    "car_label = [480, 80, 700, 170]\n",
    "all_boxes = np.array([dog_label, bike_label, car_label])\n",
    "all_ids = np.array([0, 1, 2])\n",
    "class_names = ['dog', 'bike', 'car']\n",
    "\n",
    "# see how it looks by rendering the boxes into image\n",
    "ax = utils.viz.plot_bbox(img, all_boxes, labels=all_ids, class_names=class_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LST Label for GluonCV and MXNet\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "Following the convention used in MXNet, we recommand a LST file which is a plain text list file to store labels.\n",
    "\n",
    "LST file was first introduced in MXNet following the `RecordIO design <https://mxnet.incubator.apache.org/architecture/note_data_loading.html>`_ and the `List file tutorial <https://mxnet.incubator.apache.org/faq/recordio.html>`_ of creating a LST file.\n",
    "\n",
    ".. hint::\n",
    "\n",
    "  The benefits of using single LST file are two fold:\n",
    "\n",
    "  1. It's easier to manege single file rather than scattered annotation files.\n",
    "\n",
    "  2. It's compatible with ``RecordFile`` binary format which we will cover in this tutorial later.\n",
    "\n",
    "The format of LST file is:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "integer_image_index \\t label_of_variable_length \\t relative_path_to_image\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we take the list of names of all images, shuffles them, then separates them into two lists: a training filename list and a testing filename list.\n",
    "\n",
    "Here we use compatible format for object detection task as `mxnet.image.ImageDetIter <https://mxnet.apache.org/api/python/image/image.html#image-iterator-for-object-detection>`_.\n",
    "\n",
    "`mxnet.image.ImageDetIter` is a object detection data iterator written in C++ which includes tons of augmentation choices. However, it's not flexible enough to handle all kinds of customized data augmentation.\n",
    "As a result, in GluonCV, we switched to :py:mod:`gluoncv.data.transforms` to support almost all types of data augmentations.\n",
    "\n",
    "More specifically, the label of object detection task is described as follows:\n",
    "\n",
    "![](https://github.com/dmlc/web-data/blob/master/gluoncv/datasets/detection_label.png?raw=true)\n",
    "\n",
    "\n",
    "![](https://github.com/dmlc/web-data/blob/master/gluoncv/datasets/detection_label_detail.png?raw=true)\n",
    "\n",
    "\n",
    "So, the corresponding LST file for the image we just labeled can be formatted as:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_line(img_path, im_shape, boxes, ids, idx):\n",
    "    h, w, c = im_shape\n",
    "    # for header, we use minimal length 2, plus width and height\n",
    "    # with A: 4, B: 5, C: width, D: height\n",
    "    A = 4\n",
    "    B = 5\n",
    "    C = w\n",
    "    D = h\n",
    "    # concat id and bboxes\n",
    "    labels = np.hstack((ids.reshape(-1, 1), boxes)).astype('float')\n",
    "    # normalized bboxes (recommanded)\n",
    "    labels[:, (1, 3)] /= float(w)\n",
    "    labels[:, (2, 4)] /= float(h)\n",
    "    # flatten\n",
    "    labels = labels.flatten().tolist()\n",
    "    str_idx = [str(idx)]\n",
    "    str_header = [str(x) for x in [A, B, C, D]]\n",
    "    str_labels = [str(x) for x in labels]\n",
    "    str_path = [img_path]\n",
    "    line = '\\t'.join(str_idx + str_header + str_labels + str_path) + '\\n'\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single line may be long, but contains complete information of each image required by object detection.\n",
    "\n",
    "The length of each line varies, depending on how many objects are labeled inside the corresponding image.\n",
    "\n",
    "By stacking lines one by one, it is very nature to create ``train.lst`` and ``val.lst`` for training/validation purposes.\n",
    "\n",
    "In this tutorial, we repeat the same image 4 times to create a fake ``val.lst`` file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val.lst', 'w') as fw:\n",
    "    for i in range(4):\n",
    "        line = write_line('dog.jpg', img.shape, all_boxes, all_ids, i)\n",
    "        print(line)\n",
    "        fw.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LstDetection for Loading Raw Images in Folders\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "Assume the relative root path to the image folder is current directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data import LstDetection\n",
    "lst_dataset = LstDetection('val.lst', root=os.path.expanduser('.'))\n",
    "print('length:', len(lst_dataset))\n",
    "first_img = lst_dataset[0][0]\n",
    "print('image shape:', first_img.shape)\n",
    "print('Label example:')\n",
    "print(lst_dataset[0][1])\n",
    "print(\"GluonCV swaps bounding boxes to columns 0-3 by default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RecordFileDetection for Entire Dataset Packed in Signle MXNet RecordFile\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "Reading scattered images in folders can be slow, due to constraint of disk random access speed.\n",
    "There's a significant gap between random/sequential access speed especially on HDDs.\n",
    "Even with modern PCI-E based Solid State Drives, sequential reading IO performance still blows\n",
    "random reading by a large margin.\n",
    "\n",
    "We will skip repeating the design of RecordIO built into MXNet, if you are interested, have a look at `RecordIO design <https://mxnet.incubator.apache.org/architecture/note_data_loading.html>`_.\n",
    "\n",
    "In this section, we go through the fundamental steps to create a record file.\n",
    "\n",
    "First of all, you will need a ``im2rec.py`` file to start with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. hint::\n",
    "\n",
    "     You can find `im2rec.py` in `incubator-mxnet/tools/ <https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py>`_, or you can simply download it now.\n",
    "\n",
    "     Usage:\n",
    "\n",
    "     .. code-block:: bash\n",
    "\n",
    "         python im2rec.py lst_file_name relative_root_to_images --pass-through --pack-label\n",
    "\n",
    "     Some important arguments to the ``im2rec.py``:\n",
    "\n",
    "          - ``--pass-through``: no transcode of original image, pack it to binary as is. It will preserve original quality and aspect ratio anyway.\n",
    "\n",
    "          - ``--pack-label``: pack the labels in lst file to binary record file, so ``.rec`` file is self compelete.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "im2rec = utils.download('https://raw.githubusercontent.com/apache/incubator-mxnet/' +\n",
    "                        '6843914f642c8343aaa9a09db803b6af6f5d94a2/tools/im2rec.py', 'im2rec.py')\n",
    "subprocess.check_output([sys.executable, 'im2rec.py', 'val', '.', '--no-shuffle', '--pass-through', '--pack-label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now similarly, we can create a dataset from the binary file we just created with on line of code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data import RecordFileDetection\n",
    "record_dataset = RecordFileDetection('val.rec', coord_normalized=True)\n",
    "\n",
    "# we expect same results from LstDetection\n",
    "print('length:', len(record_dataset))\n",
    "first_img = record_dataset[0][0]\n",
    "print('image shape:', first_img.shape)\n",
    "print('Label example:')\n",
    "print(record_dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Derive from PASCAL VOC format\n",
    "--------------------------------\n",
    "It you have a custom dataset fully comply with the `Pascal VOC <http://host.robots.ox.ac.uk/pascal/VOC/>`_ object detection format,\n",
    "that could be good news, because it's can be adapted to GluonCV format real quick.\n",
    "\n",
    "We provide a template for you to peek the structures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = utils.download('https://github.com/dmlc/web-data/blob/master/gluoncv/datasets/VOCtemplate.zip?raw=true', 'VOCtemplate.zip')\n",
    "with zipfile.ZipFile(fname) as zf:\n",
    "    zf.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A VOC-like dataset will have the following structure:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VOCtemplate\n",
    "└── VOC2018\n",
    "    ├── Annotations\n",
    "    │   └── 000001.xml\n",
    "    ├── ImageSets\n",
    "    │   └── Main\n",
    "    │       └── train.txt\n",
    "    └── JPEGImages\n",
    "        └── 000001.jpg\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an example of annotation file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('VOCtemplate/VOC2018/Annotations/000001.xml', 'r') as fid:\n",
    "    print(fid.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as your dataset can match the PASCAL VOC convension, it is convenient to\n",
    "derive custom dataset from ``VOCDetection``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data import VOCDetection\n",
    "class VOCLike(VOCDetection):\n",
    "    CLASSES = ['person', 'dog']\n",
    "    def __init__(self, root, splits, transform=None, index_map=None, preload_label=True):\n",
    "        super(VOCLike, self).__init__(root, splits, transform, index_map, preload_label)\n",
    "\n",
    "dataset = VOCLike(root='VOCtemplate', splits=((2018, 'train'),))\n",
    "print('length of dataset:', len(dataset))\n",
    "print('label example:')\n",
    "print(dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last column indicate the difficulties of labeled object\n",
    "You can ignore the following section if it's out of your intention in the xml file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"<difficult>0</difficult>\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
