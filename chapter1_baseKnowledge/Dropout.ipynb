{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MXNet实现正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mxnet import gluon,init,autograd,nd\n",
    "from mxnet.gluon import data as gdata,loss as gloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义一个dropout层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X,drop_prob):\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    \n",
    "    keep_prob = 1-drop_prob\n",
    "    \n",
    "    if keep_prob == 0:\n",
    "        return X.zero_likes()\n",
    "    \n",
    "    mask = nd.random.uniform(0,1,X.shape,ctx = mx.gpu()) < keep_prob\n",
    "    \n",
    "    return mask*X/keep_prob   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nd.arange(16,ctx = mx.gpu()).reshape(2,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11. 12. 13. 14. 15.]]\n",
      "<NDArray 2x8 @gpu(0)>\n",
      "\n",
      "[[ 0.  2.  4.  0.  0. 10. 12.  0.]\n",
      " [ 0.  0.  0. 22.  0. 26.  0.  0.]]\n",
      "<NDArray 2x8 @gpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(dropout(X,0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs,num_outputs,num_hidden1,num_hidden2 =  784,10,256,256\n",
    "\n",
    "#定义两个隐层\n",
    "W1 = nd.random.normal(scale = 0.01,shape = (num_inputs,num_hidden1),ctx = mx.gpu())\n",
    "b1 = nd.zeros(shape = (1,num_hidden1),ctx = mx.gpu())\n",
    "\n",
    "W2 = nd.random.normal(scale = 0.01,shape = (num_hidden1,num_hidden2),ctx = mx.gpu())\n",
    "b2 = nd.zeros(shape=(1,num_hidden2),ctx = mx.gpu())\n",
    "#定义输出层\n",
    "W3 = nd.random.normal(scale=0.01,shape = (num_hidden2,num_outputs),ctx = mx.gpu())\n",
    "b3 = nd.zeros(shape = (1,num_outputs),ctx = mx.gpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [W1,b1,W2,b2,W3,b3]\n",
    "\n",
    "for param in params :\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1,drop_prob2 = 0.2,0.5\n",
    "def net(X):\n",
    "    X = X.reshape(-1,num_inputs)\n",
    "    H1 = (nd.dot(X,W1)+b1).relu()      #第一个隐藏层的输出\n",
    "    \n",
    "    #只在训练的时候进行丢弃\n",
    "    if autograd.is_training():\n",
    "        H1 = dropout(H1,drop_prob1)   #第一个隐层后面添加丢弃层\n",
    "    H2 = (nd.dot(H1,W2)+b2).relu()\n",
    "    \n",
    "    if autograd.is_training():\n",
    "        H2 = dropout(H2,drop_prob2)   #第二个隐层之后添加丢弃层\n",
    "    return (nd.dot(H2,W3)+b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params,batch_size,learning_rate):\n",
    "    for param in params:\n",
    "        param[:] = param-learning_rate*param.grad/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs ,lr,batch_size = 5,0.5,256\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "def ToTensor(X,y):\n",
    "    return X.astype(np.float32)/255,y.astype(np.float32)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "with mx.Context(mx.gpu()):\n",
    "    trainData = gdata.vision.FashionMNIST(root = './FashionMNIST',train=True,transform = ToTensor)\n",
    "    testData = gdata.vision.FashionMNIST(root ='./FashionMNIST',train = False,transform = ToTensor)\n",
    "    \n",
    "    train_iter = gdata.DataLoader(trainData,batch_size,shuffle=True)\n",
    "    test_iter = gdata.DataLoader(testData,batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat,y):\n",
    "    return (y_hat.argmax(axis = 1)==y).mean().asscalar()\n",
    "\n",
    "def evaluate_accuracy(net,data):\n",
    "    acc = 0\n",
    "    for X,y in data:\n",
    "        y_hat = net(X)\n",
    "        acc += accuracy(y_hat,y)\n",
    "    return acc/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_drop_out_net(net,train_iter,test_iter,num_epochs,batch_size,loss,trainer=None,params=None,lr=None):\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        train_loss_sum = 0\n",
    "        train_acc_sum = 0\n",
    "        for X,y in train_iter :\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat,y)\n",
    "            l.backward()\n",
    "            \n",
    "            if trainer:\n",
    "                trainer.step(batch_size)\n",
    "            else:\n",
    "                sgd(params,batch_size,lr)\n",
    "            \n",
    "            train_loss_sum += l.mean().asscalar()\n",
    "            train_acc_sum += accuracy(y_hat,y)\n",
    "        test_acc = evaluate_accuracy(net,test_iter)\n",
    "        print('epoch %d train_loss %.4f train_acc %.3f test_acc %.3f'\n",
    "             %(epoch,train_loss_sum/len(train_iter),train_acc_sum/len(train_iter),test_acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train_loss 1.1539 train_acc 0.554 test_acc 0.796\n",
      "epoch 3 train_loss 0.5899 train_acc 0.781 test_acc 0.828\n",
      "epoch 4 train_loss 0.5031 train_acc 0.818 test_acc 0.845\n",
      "epoch 5 train_loss 0.4520 train_acc 0.836 test_acc 0.865\n",
      "epoch 6 train_loss 0.4260 train_acc 0.845 test_acc 0.866\n"
     ]
    }
   ],
   "source": [
    "with mx.Context(mx.gpu()):\n",
    "    train_drop_out_net(net,train_iter,test_iter,num_epochs,batch_size,loss,trainer=None,params=params,lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Gluon接口实现dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256,activation='relu'),\n",
    "        nn.Dropout(drop_prob2),               \n",
    "        nn.Dense(256,activation='relu'),\n",
    "        nn.Dropout(drop_prob1),\n",
    "        nn.Dense(10)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize(init.Normal(sigma=0.01),ctx=mx.gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 train_loss 1.1716 train_acc 0.544 test_acc 0.775\n",
      "epoch 3 train_loss 0.5913 train_acc 0.778 test_acc 0.823\n",
      "epoch 4 train_loss 0.5089 train_acc 0.812 test_acc 0.854\n",
      "epoch 5 train_loss 0.4647 train_acc 0.830 test_acc 0.858\n",
      "epoch 6 train_loss 0.4427 train_acc 0.839 test_acc 0.854\n"
     ]
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':lr})\n",
    "with mx.Context(mx.gpu()):\n",
    "    train_drop_out_net(net,train_iter,test_iter,num_epochs,batch_size,loss,trainer=trainer,lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
