{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN的Gluon实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluonbook as gb\n",
    "import math\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn, rnn\n",
    "import time\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(corpus_indices,char_to_idx,idx_to_char,\n",
    "vocab_size) = gb.load_data_jay_lyrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 下面构造一个单隐藏层、隐藏单元个数为256的rnn，并对权重进行初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "rnn_layer = rnn.RNN(num_hiddens)\n",
    "rnn_layer.initialize(ctx= mx.gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 接下来调用begin_state来返回初始化的隐藏状态列表。它有一个形状为 (隐藏层个数，批量大小，隐藏单元个数)的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 256)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "state = rnn_layer.begin_state(batch_size = batch_size,ctx= mx.gpu())\n",
    "state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35, 2, 256), 1, (1, 2, 256))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_steps = 35\n",
    "X = nd.random.uniform(shape=(num_steps,batch_size,vocab_size),ctx = mx.gpu())\n",
    "Y ,state_new = rnn_layer(X,state)\n",
    "Y.shape,len(state_new),state_new[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下面继承Block类来定义一个完整的循环神经网络。它首先将输入数据使用one-hot向量表示后输入到rnn-layer中，然后使用全连接层得到输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Block):\n",
    "    def __init__(self,rnn_layer,vocab_size,**kwargs):\n",
    "        super(RNNModel,self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dense = nn.Dense(vocab_size)\n",
    "    \n",
    "    def forward(self,input,state):\n",
    "        #将输入转置成(num_steps,batch_size)后获取one-hot表示\n",
    "        X = nd.one_hot(input.T,self.vocab_size)\n",
    "        Y,state = self.rnn(X,state)\n",
    "        # 全连接层会⾸先将 Y 的形状变成（num_steps * batch_size， num_hiddens），\n",
    "        # 它的输出形状为（num_steps * batch_size， vocab_size）。\n",
    "        output = self.dense(Y.reshape((-1,Y.shape[-1])))\n",
    "        return output,state\n",
    "    \n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * 定义一个预测函数。这里的实现区别在于前向计算和初始化隐藏状态的函数接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn_gluon(prefix,num_chars,model,vocab_size,ctx,idx_to_char,char_to_idx):\n",
    "    #初始化隐藏层状态\n",
    "    state = model.begin_state(batch_size  =1,ctx=ctx)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    \n",
    "    for t in range(num_chars+len(prefix)-1):\n",
    "        #上一次的输出作为这一次的输入\n",
    "        X = nd.array([output[-1]],ctx).reshape((1,1))\n",
    "        (Y,state) = model(X,state) #前向运算,里面进行了one-hot编码\n",
    "        if t <len(prefix)-1:\n",
    "            output.append(char_to_idx[prefix[t+1]])\n",
    "        else:\n",
    "            output.append(int(Y.argmax(axis=1).asscalar()))\n",
    "    return ''.join([idx_to_char[i] for i in output])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开深谢腿耍连学进漠泛飘'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = gb.try_gpu()\n",
    "model = RNNModel(rnn_layer,vocab_size)\n",
    "model.initialize(force_reinit=True,ctx=ctx)\n",
    "predict_rnn_gluon('分开',10,model,vocab_size,ctx,idx_to_char,char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 接下来实现训练函数，这里使用了随机采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_rnn_gluon(model,num_hiddens,vocab_size,ctx,\n",
    "                               corpus_indices,idx_to_char,char_to_idx,\n",
    "                               num_epochs,num_steps,lr,clipping_theta,\n",
    "                               batch_size,pred_period,pred_len,prefixes):\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    model.initialize(ctx=ctx,init=init.Normal(0.01),force_reinit=True)\n",
    "    trainer = gluon.Trainer(model.collect_params(),'sgd',{'learning_rate':lr,'momentum':0,'wd':0})\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loss_sum,start = 0.0,time.time()\n",
    "        data_iter = gb.data_iter_random(corpus_indices,batch_size,num_steps,ctx=ctx)\n",
    "        #state = model.begin_state(batch_size=batch_size,ctx=ctx)\n",
    "        for t,(X,Y) in enumerate(data_iter):\n",
    "            state = model.begin_state(batch_size=batch_size,ctx=ctx)\n",
    "#             for s in state:\n",
    "#                 s.detach()\n",
    "            with autograd.record():\n",
    "                (output,state) = model(X,state)\n",
    "                y = Y.T.reshape((-1,))\n",
    "                l = loss(output,y).mean()\n",
    "            l.backward()\n",
    "            #梯度裁剪\n",
    "            params = [p.data() for p in model.collect_params().values()]\n",
    "            gb.grad_clipping(params,clipping_theta,ctx)\n",
    "            trainer.step(1)\n",
    "            loss_sum+=l.asscalar()\n",
    "        if (epoch+1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f,time %.2f sec'%\n",
    "                 (epoch+1,math.exp(loss_sum/(t+1)),time.time()-start))\n",
    "            for prefix in prefixes:\n",
    "                print('-',predict_rnn_gluon(prefix,pred_len,model,vocab_size,ctx,idx_to_char,char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, perplexity 18.647071,time 0.07 sec\n",
      "- 分开 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我\n",
      "- 不分开  我有你这里很 后知后觉 我该好好生活 后知后觉 你已经离开我 不知不觉 我跟好这生活 后知后觉 \n",
      "epoch 200, perplexity 3.092964,time 0.07 sec\n",
      "- 分开 有什么 干什么 我吸就这样牵着你的手不放开 爱可不能够永简单纯没有悲哀 我 想带你骑单车 默 还不\n",
      "- 不分开扫 我不要再想 我不 我想 我不要再想你 不知不觉 你已经离开我 已知不觉 不知了觉截棍 哼哼哈兮 \n",
      "epoch 300, perplexity 1.848787,time 0.07 sec\n",
      "- 分开的凯萨琳公主泊专 离不开暴风圈来不及逃 我不能再想 我不能再想 我不要再想 我不要再想 我不要再想 \n",
      "- 不分开扫 我叫能爸想 我不要再想 我不能再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 \n",
      "epoch 400, perplexity 1.554516,time 0.08 sec\n",
      "- 分开 默默不碎 泣不再 一壶令它心仪的母斑鸠 牛仔红蕃 在小镇 背对背决斗 一只灰狼 问候伦 废墟背囱 \n",
      "- 不分开扫 然后将过去 慢慢温习 让我该恨都难以美后 将真心抽离写成日记 像是一场默剧 美无法被安排的雨 有\n",
      "epoch 500, perplexity 1.415595,time 0.08 sec\n",
      "- 分开 娘子已人三千七百多年 你在橱窗前 说视碑文的字眼 我却在旁静静欣赏 就想躲 说你眼睛看着我 不知不\n",
      "- 不分开吗 我不能再想 我不 我不 我不要再想你 不知不觉 你已经离开我 不知不觉 我跟了这节奏 后知后人 \n",
      "epoch 600, perplexity 1.417990,time 0.07 sec\n",
      "- 分开 宁愿已经三千七百多年 你在橱窗前 凝视碑文的字眼 我却在旁静静欣赏后还会够 它色后的 快时光中的没\n",
      "- 不分开期 我叫你爸 你打我妈 这样对吗干嘛这样 从必让酒牵鼻成走记 像是在场只敌 你的完美主义 太彻底 分\n",
      "epoch 700, perplexity 1.320383,time 0.08 sec\n",
      "- 分开 周今心 一步两武术的老板 练铁沙掌 耍杨家枪 硬底子功夫最擅长 还入下危险边缘Baby  我的世界\n",
      "- 不分开吗 我叫你爸让鸠我的愿望一只经小 我怎么每不 凝一布实口 仙子掌怕羞 蜥蝪横著走 这里什么奇怪的事都\n",
      "epoch 800, perplexity 1.281357,time 0.07 sec\n",
      "- 分开 周杰已经 快使用双截棍 哼哼哈兮 快使我有轻功 飞檐走壁 为人耿直不屈 一身正气 快使用双截棍 哼\n",
      "- 不分开简简的胖女巫 用拉丁文念咒语啦啦呜 她养的黑猫笑起来像哭 啦啦啦呜 阻了之 怎么我有心人着  昨晚没\n",
      "epoch 900, perplexity 1.306309,time 0.07 sec\n",
      "- 分开 周杰我 我跟开  回前不觉 你对懂离 我面轻带宠 我的天空 是雨是风 还是彩虹 你在操纵 我我不带\n",
      "- 不分开吗 然后将过去 慢慢温习 温不了空屋 白色蜡烛 温暖了空屋 白色蜡烛 温暖了空屋 白色蜡烛 温暖了空\n",
      "epoch 1000, perplexity 1.255974,time 0.07 sec\n",
      "- 分开 干什么 这行脚 呼吸吐纳心自在 干什么 干什么 气沉丹田手心开 干什么 干什么 我打开任督二脉 干\n",
      "- 不分开期 我不能再想 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受我已无处可躲就耳濡目染 什么刀枪\n"
     ]
    }
   ],
   "source": [
    "num_epochs, batch_size, lr, clipping_theta = 1000, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 100, 50, ['分开', '不分开']\n",
    "train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,\n",
    "                            corpus_indices, idx_to_char, char_to_idx,\n",
    "                            num_epochs, num_steps, lr, clipping_theta,\n",
    "                            batch_size, pred_period, pred_len, prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gluon实现速度快了很多很多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
