{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现一个早期用来识别手写数字图像的卷积神经网络LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet分为卷积层和全连接层两个部分\n",
    "+ 其中卷积层的基本单位是卷积层后接最大池化层\n",
    "    - 卷积层用来识别图像里的空间模式\n",
    "    - 最大池化层用来降低卷积层对位置的敏感性\n",
    "+ 全连接层会将小批量中的每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维为小批量中的样本，第二维为每个样本变平后表示的向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用Sequential类来实现 LeNet模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gluonbook as gb\n",
    "import mxnet as mx \n",
    "from mxnet.gluon import data as gdata,nn,loss as gloss\n",
    "from mxnet import nd,autograd,init,gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet = nn.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建LeNet模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet.add(nn.Conv2D(channels=8,kernel_size=5,activation='relu'),  #第一个卷积层 输出通道数为6，卷积核大小为5*5\n",
    "          nn.MaxPool2D(pool_size=2,strides=2),                        #后面接一个池化层，降低控件敏感度\n",
    "          nn.Conv2D(channels=20,kernel_size=3,activation='relu'),\n",
    "          nn.MaxPool2D(pool_size =2,strides=2),\n",
    "          \n",
    "          #下面接全连接层，Dense会默认将(批量大小，通道，高，宽)形状的输入转换成\n",
    "          #（批量大小，通道*高*宽）形状的输入\n",
    "          nn.Dense(240,activation='relu'),\n",
    "          nn.Dense(120,activation='relu'),\n",
    "          nn.Dense(10)                \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 观察每一层的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(X,net):\n",
    "    for layer in net:\n",
    "        X = layer(X)\n",
    "        print(layer.name,'output shape:\\t',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv0 output shape:\t (1, 8, 24, 24)\n",
      "pool0 output shape:\t (1, 8, 12, 12)\n",
      "conv1 output shape:\t (1, 20, 10, 10)\n",
      "pool1 output shape:\t (1, 20, 5, 5)\n",
      "dense0 output shape:\t (1, 240)\n",
      "dense1 output shape:\t (1, 120)\n",
      "dense2 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1,1,28,28),ctx=mx.gpu())\n",
    "LeNet.initialize(ctx=mx.gpu())\n",
    "\n",
    "print_output(X,LeNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  获取数据和训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size  = 256\n",
    "train_iter,test_iter = gb.load_data_fashion_mnist(batch_size,root='../chapter1_baseKnowledge/FashionMNIST/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu4():\n",
    "    try:\n",
    "        ctx=mx.gpu()\n",
    "        _ = nd.zeros((1,),ctx=ctx)\n",
    "    except mx.base.MXNetError:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = try_gpu4()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在GPU中训练\n",
    "def evaluate_accuracy(data_iter,net,ctx):\n",
    "    acc = nd.array([0],ctx= ctx)\n",
    "    for X,y in data_iter:\n",
    "        X,y = X.as_in_context(ctx),y.as_in_context(ctx)\n",
    "        acc+=gb.accuracy(net(X),y)\n",
    "    return acc.asscalar()/len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LeNet_on_GPU(net,train_iter,test_iter,batch_size,trainer,ctx,num_epochs):\n",
    "    print('training on ',ctx)\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,start = 0,0,time.time()\n",
    "        for X,y in train_iter:\n",
    "            X =X.as_in_context(ctx)\n",
    "            y =y.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat,y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_l_sum += l.mean().asscalar()\n",
    "            train_acc_sum += gb.accuracy(y_hat,y)\n",
    "        \n",
    "        test_acc = evaluate_accuracy(test_iter,net,ctx)\n",
    "    \n",
    "        print('epoch %d,loss %.4f,train acc %.3f,test acc %.3f,'\n",
    "              'time %.1f sec'\n",
    "             %(epoch+1,train_l_sum/len(train_iter),train_acc_sum/len(train_iter),test_acc,time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr,num_epochs = 0.1,5\n",
    "LeNet.initialize(force_reinit=True,ctx=ctx,init=init.Xavier())\n",
    "trainer = gluon.Trainer(LeNet.collect_params(),'sgd',{'learning_rate':lr})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU训练速度大概是CPU 5倍 ，relu为激活函数的CNN 学习率要设置的小一点，不然不会收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  gpu(0)\n",
      "epoch 1,loss 0.9342,train acc 0.652,test acc 0.785,time 5.3 sec\n",
      "epoch 2,loss 0.5306,train acc 0.799,test acc 0.832,time 5.4 sec\n",
      "epoch 3,loss 0.4572,train acc 0.830,test acc 0.855,time 5.3 sec\n",
      "epoch 4,loss 0.4096,train acc 0.850,test acc 0.866,time 5.4 sec\n",
      "epoch 5,loss 0.3776,train acc 0.861,test acc 0.874,time 5.4 sec\n",
      "epoch 6,loss 0.3546,train acc 0.871,test acc 0.880,time 5.3 sec\n",
      "epoch 7,loss 0.3394,train acc 0.875,test acc 0.882,time 5.3 sec\n",
      "epoch 8,loss 0.3215,train acc 0.882,test acc 0.883,time 5.3 sec\n",
      "epoch 9,loss 0.3096,train acc 0.888,test acc 0.887,time 5.3 sec\n",
      "epoch 10,loss 0.2995,train acc 0.889,test acc 0.891,time 5.4 sec\n",
      "epoch 11,loss 0.2918,train acc 0.894,test acc 0.887,time 5.3 sec\n",
      "epoch 12,loss 0.2819,train acc 0.897,test acc 0.897,time 5.4 sec\n",
      "epoch 13,loss 0.2706,train acc 0.899,test acc 0.896,time 5.4 sec\n",
      "epoch 14,loss 0.2660,train acc 0.901,test acc 0.890,time 5.4 sec\n",
      "epoch 15,loss 0.2567,train acc 0.905,test acc 0.901,time 5.3 sec\n",
      "epoch 16,loss 0.2494,train acc 0.908,test acc 0.904,time 5.2 sec\n",
      "epoch 17,loss 0.2457,train acc 0.908,test acc 0.896,time 5.3 sec\n",
      "epoch 18,loss 0.2374,train acc 0.911,test acc 0.902,time 5.3 sec\n",
      "epoch 19,loss 0.2296,train acc 0.914,test acc 0.902,time 5.2 sec\n",
      "epoch 20,loss 0.2233,train acc 0.918,test acc 0.905,time 5.4 sec\n",
      "epoch 21,loss 0.2210,train acc 0.918,test acc 0.897,time 5.3 sec\n",
      "epoch 22,loss 0.2142,train acc 0.920,test acc 0.908,time 5.3 sec\n",
      "epoch 23,loss 0.2091,train acc 0.921,test acc 0.907,time 5.3 sec\n",
      "epoch 24,loss 0.2062,train acc 0.923,test acc 0.909,time 5.2 sec\n",
      "epoch 25,loss 0.1999,train acc 0.925,test acc 0.910,time 5.4 sec\n",
      "epoch 26,loss 0.1935,train acc 0.928,test acc 0.901,time 5.3 sec\n",
      "epoch 27,loss 0.1942,train acc 0.928,test acc 0.909,time 5.3 sec\n",
      "epoch 28,loss 0.1829,train acc 0.932,test acc 0.911,time 5.3 sec\n",
      "epoch 29,loss 0.1818,train acc 0.933,test acc 0.911,time 5.3 sec\n",
      "epoch 30,loss 0.1755,train acc 0.935,test acc 0.909,time 5.3 sec\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "train_LeNet_on_GPU(LeNet,train_iter,test_iter,batch_size,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
