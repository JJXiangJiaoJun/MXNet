{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 残差网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按理来说，对神经网络模型添加新的层，充分训练后的模型是否可能更加有效地降低训练误差？\n",
    "理论上，原模型的解空间只是新模型解的空间的子空间。也就是说，如果我们能将新添加的层训练成恒等映射 $f(x) = x$ ,那么原模型和新模型是同样有效地。由于新模型可能得出更优的解来拟合训练数据集，添加层似乎更容易降低训练误差。然而在实践中，添加过多的层后训练误差往往不降反升。即使利⽤批量归⼀化带来的数值稳定性使得训练深层模型更加容易，这个问题仍然存在。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 残差块\n",
    "在残差快中，输入可通过跨层的数据线路更快地向前传播\n",
    "+ ResNet沿用了VGG全 3\\*3 卷积层的设计。残差块中首先有两个同样输出通道数的 3\\*3卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。然后我们将输入跳过这两个卷积运算后直接加在最后的ReLU激活函数前。这样的设计要求两个卷积层的输入输出形状一样，从而可以相加。如果需要改变通道数，可以通过引入1\\*1卷积层来将输入变换成需要的形状后再做相加运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 残差块，可以设定输出通道数、是否使用额外的1\\*1卷积层来修改通道数，以及卷积的步幅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gluonbook as gb\n",
    "import mxnet as mx\n",
    "from mxnet import nd,autograd,init\n",
    "from mxnet.gluon import nn,data as gdata,loss as gloss\n",
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Block):\n",
    "    def __init__(self,num_channels,use_1x1conv=False,strides=1,**kwargs):\n",
    "        super(Residual,self).__init__(**kwargs)\n",
    "        self.conv1 = nn.Conv2D(num_channels,kernel_size=3,strides=strides,padding=1)\n",
    "        self.bn1 = nn.BatchNorm()\n",
    "        #第二个卷积层步幅固定为1，如果两个卷积都减小尺寸，后面将出现问题\n",
    "        self.conv2 = nn.Conv2D(num_channels,kernel_size=3,strides=1,padding=1)\n",
    "        self.bn2 = nn.BatchNorm()\n",
    "        \n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2D(num_channels,kernel_size=1,strides=strides)\n",
    "        else:\n",
    "            self.conv3=None\n",
    "    #实现变形操作，批量归一化 -> 激活 -> 卷积\n",
    "#     def forward(self,X):\n",
    "#         Y = nd.relu(self.bn1(self.conv1(X)))\n",
    "#         Y = self.bn2(self.conv2(Y))\n",
    "#         if self.conv3:\n",
    "#             X = self.conv3(X)\n",
    "#         return nd.relu(Y+X)\n",
    "    #实现变形操作，批量归一化 -> 激活 -> 卷积\n",
    "    def forward(self,X): \n",
    "        Y = self.conv1(nd.relu(self.bn1(X)))\n",
    "        Y = self.conv2(nd.relu(self.bn2(Y)))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(nd.relu(X))\n",
    "        return Y+X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 96, 96)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3,use_1x1conv=True)\n",
    "blk.initialize(ctx=mx.gpu())\n",
    "X = nd.random.uniform(shape=(1,1,96,96),ctx=mx.gpu())\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 和GoogLeNet一样，前两层为7\\*7输出通道为64、步幅为2的卷积层，后面接步幅为2的3\\*3的最大池化层，不过每个卷积层后都增加了批量归一化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet = nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet.add(nn.Conv2D(channels=32,kernel_size=7,strides=2,padding=3),\n",
    "           nn.BatchNorm(),nn.Activation('relu'),\n",
    "           nn.MaxPool2D(pool_size=3,strides=2,padding=1)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GoogLeNet在后面接了由4个Inception块组成的模块。ResNet则使用了四个有残差块组成的模块，每个模块使用了若干个同样输出通道数的残差块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(num_channels,num_residuals,first_block=False):\n",
    "    blk  = nn.Sequential()\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.add(Residual(num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet.add(resnet_block(32,2,first_block=True),\n",
    "           resnet_block(64,2),\n",
    "           resnet_block(128,2),\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 最后像GoogLeNet一样加入全局池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNet.add(nn.GlobalAvgPool2D(),nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 conv6 output shape\t (1, 32, 112, 112)\n",
      "2 batchnorm4 output shape\t (1, 32, 112, 112)\n",
      "3 relu0 output shape\t (1, 32, 112, 112)\n",
      "4 pool0 output shape\t (1, 32, 56, 56)\n",
      "5 sequential1 output shape\t (1, 32, 56, 56)\n",
      "6 sequential2 output shape\t (1, 64, 28, 28)\n",
      "7 sequential3 output shape\t (1, 128, 14, 14)\n",
      "8 pool1 output shape\t (1, 128, 1, 1)\n",
      "9 dense0 output shape\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1,1,224,224),ctx=mx.gpu())\n",
    "ResNet.initialize(ctx=mx.gpu(),force_reinit=True)\n",
    "cnt=0\n",
    "for layer in ResNet:\n",
    "    try:\n",
    "        cnt +=1\n",
    "        X = layer(X)\n",
    "        print(cnt,layer.name,'output shape\\t',X.shape)\n",
    "    except mx.base.MXNetError as e:\n",
    "        print('Error!\\t',layer.name)\n",
    "        print(X.shape)\n",
    "        print(e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取数据并且训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr,num_epochs,batch_size,ctx = 0.1,10,256,gb.try_gpu()\n",
    "trainer = gluon.Trainer(ResNet.collect_params(),'sgd',{'learning_rate':lr})\n",
    "ResNet.initialize(init=init.Xavier(),ctx=ctx,force_reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter,test_iter = gb.load_data_fashion_mnist(batch_size,resize=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on gpu(0)\n",
      "epoch 1, loss 1.1860, train acc 0.692, test acc 0.841, time 41.7 sec\n",
      "epoch 2, loss 0.3766, train acc 0.860, test acc 0.881, time 41.5 sec\n",
      "epoch 3, loss 0.2996, train acc 0.889, test acc 0.898, time 42.1 sec\n",
      "epoch 4, loss 0.2597, train acc 0.905, test acc 0.904, time 41.4 sec\n",
      "epoch 5, loss 0.2302, train acc 0.916, test acc 0.905, time 41.1 sec\n",
      "epoch 6, loss 0.2049, train acc 0.925, test acc 0.911, time 40.9 sec\n",
      "epoch 7, loss 0.1858, train acc 0.932, test acc 0.912, time 40.8 sec\n",
      "epoch 8, loss 0.1667, train acc 0.939, test acc 0.913, time 41.1 sec\n",
      "epoch 9, loss 0.1518, train acc 0.944, test acc 0.917, time 42.3 sec\n",
      "epoch 10, loss 0.1376, train acc 0.949, test acc 0.908, time 41.3 sec\n"
     ]
    }
   ],
   "source": [
    "gb.train_ch5(ResNet,train_iter,test_iter,batch_size,trainer,ctx,num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可以发现ResNet收敛速度特别快，基本在第一个epoch完成后就收敛了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 卷积层+批量归一化+激活 训练结果如下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* training on gpu(0)\n",
    "* epoch 1, loss 0.5569, train acc 0.796, test acc 0.862, time 41.9 sec\n",
    "* epoch 2, loss 0.3092, train acc 0.886, test acc 0.900, time 41.0 sec\n",
    "* epoch 3, loss 0.2519, train acc 0.908, test acc 0.886, time 41.2 sec\n",
    "* epoch 4, loss 0.2204, train acc 0.919, test acc 0.884, time 42.1 sec\n",
    "* epoch 5, loss 0.1946, train acc 0.929, test acc 0.901, time 42.0 sec\n",
    "* epoch 6, loss 0.1743, train acc 0.937, test acc 0.896, time 41.1 sec\n",
    "* epoch 7, loss 0.1541, train acc 0.944, test acc 0.912, time 40.3 sec\n",
    "* epoch 8, loss 0.1375, train acc 0.950, test acc 0.917, time 40.9 sec\n",
    "* epoch 9, loss 0.1216, train acc 0.955, test acc 0.904, time 40.6 sec\n",
    "* epoch 10, loss 0.1071, train acc 0.961, test acc 0.893, time 41.5 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用批量归一化+激活+卷积 训练结果如下所示\n",
    "\n",
    "论文中主要是 主要是改进模型，更好训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
