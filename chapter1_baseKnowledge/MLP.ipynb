{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多层感知机实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mxnet import nd,gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet.gluon.data as gdata\n",
    "import mxnet.gluon.loss as gloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 首先加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将图像数据进行归一化\n",
    "def ToTensor(X,y):\n",
    "    return X.astype(np.float32)/255,y\n",
    "\n",
    "ctx = mx.gpu()\n",
    "batch_size = 256\n",
    "with mx.Context(ctx):\n",
    "    trainData = gdata.vision.FashionMNIST(root=\"./FashionMNIST\",train=True,transform=ToTensor)\n",
    "    testData  = gdata.vision.FashionMNIST(root=\"./FashionMNIST\",train=False,transform=ToTensor)\n",
    "    train_iter = gdata.DataLoader(trainData,batch_size,shuffle = True)\n",
    "    test_iter = gdata.DataLoader(testData,batch_size,shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下面进行模型的搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 首先定义 $Relu()$函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型参数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "num_hiddens = 256\n",
    "with ctx:\n",
    "    W1 = nd.random.normal(scale=0.01,shape=(num_inputs,num_hiddens))\n",
    "    b1 = nd.zeros(num_hiddens)\n",
    "\n",
    "    W2 = nd.random.normal(scale=0.01,shape=(num_hiddens,num_outputs))\n",
    "    b2 = nd.zeros(num_outputs)\n",
    "\n",
    "    params = [W1,b1,W2,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型   输入层、隐藏层、输出层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X = X.reshape((-1,num_inputs))\n",
    "    H = relu(nd.dot(X,W1)+b1)\n",
    "    return nd.dot(H,W2)+b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gloss.SoftmaxCrossEntropyLoss()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机梯度下降算法\n",
    "def sgd(params,lr,batch_size):\n",
    "    for param in params:\n",
    "        param[:] = param[:]-lr*param[:].grad/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行模型的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#评估模型的准确度\n",
    "def accuracy(y_hat,y):\n",
    "    return (y_hat.argmax(axis=1) == y.astype('float32')).mean().asscalar()\n",
    "\n",
    "def evaluate_accuracy(data_iter,net):\n",
    "    acc = 0\n",
    "    for X,y in data_iter:\n",
    "        acc+=accuracy(net(X),y)\n",
    "    return acc/len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "def train_mlp(net,train_iter,test_iter,loss,num_epochs,batch_size,params,lr):\n",
    "    #\n",
    "    for epoch in range(num_epochs+1):\n",
    "        train_l = 0\n",
    "        train_acc = 0\n",
    "        #首先取出小批量数据\n",
    "        for X,y in train_iter:\n",
    "            with autograd.record():\n",
    "                y_hat = net(X)      #前向运算\n",
    "                l = loss(y_hat,y)   #计算损失函数\n",
    "            l.backward()           #反向传播、进行求导运算\n",
    "            \n",
    "            #使用sgd对参数进行优化\n",
    "            sgd(params,lr,batch_size)\n",
    "            \n",
    "            #记录损失\n",
    "            train_l += l.mean().asscalar()\n",
    "            train_acc += accuracy(y_hat,y)\n",
    "        \n",
    "        #完成了一个epoch训练，进行一次测试并且输出结果\n",
    "        \n",
    "        test_acc = evaluate_accuracy(test_iter,net)\n",
    "        \n",
    "        #打印日志\n",
    "        print('epoch %d train loss %.4f train acc %.3f test acc %.3f'\n",
    "              %(epoch+1,train_l/len(train_iter),train_acc/len(train_iter),test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train loss 0.8761 train acc 0.715 test acc 0.829\n",
      "epoch 2 train loss 0.4946 train acc 0.818 test acc 0.848\n",
      "epoch 3 train loss 0.4452 train acc 0.835 test acc 0.855\n",
      "epoch 4 train loss 0.4090 train acc 0.849 test acc 0.862\n",
      "epoch 5 train loss 0.3870 train acc 0.857 test acc 0.872\n",
      "epoch 6 train loss 0.3713 train acc 0.863 test acc 0.870\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "lr =0.5\n",
    "with ctx:\n",
    "    train_mlp(net,train_iter,test_iter,loss,num_epochs,batch_size,params,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0 1 2 2 3 2 8 6 5 0 3 4 4 6 8 5 6 3 6 4 4 4 2 1 5 7 8 4 4 1 5 7 7 8 1 0 9\n",
      " 8 0 8 2 0 4 6 2 0 3 3 2 3 2 2 9 3 0 9 9 4 6 0 4 5 4 6 1 1 0 9 5 2 7 3 4 6\n",
      " 5 7 1 6 1 4 9 8 1 2 4 8 9 4 1 6 3 4 2 2 2 6 4 7 7 3 9 3 9 0 8 2 3 8 2 7 5\n",
      " 5 3 2 7 5 0 2 7 1 0 5 4 4 7 0 8 5 0 3 1 7 9 4 9 6 4 4 2 4 3 3 3 2 2 6 0 0\n",
      " 1 3 4 3 3 1 9 3 3 3 9 5 6 7 7 3 2 4 0 8 7 2 2 8 9 0 2 4 4 5 7 9 9 1 3 9 1\n",
      " 5 5 6 0 7 4 9 1 6 0 0 0 4 0 9 0 4 2 5 5 8 6 2 1 9 0 4 7 1 9 5 9 0 2 8 5 7\n",
      " 7 3 2 4 5 7 8 1 9 5 6 2 9 7 4 0 9 2 1 5 7 7 0 2 4 5 3 3 8 1 6 2 4 8]\n",
      "<NDArray 256 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for X,y in test_iter:\n",
    "    print (y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
